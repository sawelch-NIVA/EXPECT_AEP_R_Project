---
title: "Notebook 02 - Vannmiljo Conversion"
author: "Sam Welch"
date: "2025.12.16"
---

This notebook summarises and reports on the process of importing, cleaning, and reformatting data from the Norwegian Environmental Ministry's Vannmiljø database.

# To Do:
-   [ ] Investigate some very strange detection and quantification limits
-   [ ] Add probable contamination source
-   [ ] More sanity checking of extracted values
-   [ ] @fig-vm-dotplot - what's going on here?
-   [ ] Move boring code to targets
-   [ ] Fix export **again**.

# Libraries & Setup

Import required packages and set working directory to project root.

```{r}
#| label: setup-libraries
#| warning: false
#| message: false

knitr::opts_chunk$set(dev = "svglite") # otherwise some SVGs cause problems in firefox?

library(dplyr)
library(tidyr)
library(stringr)
library(lubridate)
library(readr)
library(readxl)
library(glue)
library(ggplot2)
library(sf)
library(DT)
library(summarytools)
library(STOPeData)
library(eDataDRF)
library(here)
library(knitr)
library(devtools)
library(tibble)

load_all()

i_am("README.md") |> suppressMessages()
```

## Configuration

See @tbl-config-filters and @tbl-config-metadata for a summary of the parameters of this extraction from Vannmiljø.

```{r}
#| label: setup-configuration
#| warning: false

config <- list(
  # File paths
  files = list(
    # TODO: Make imports more consistent and timestamp everything
    vm_copper = here("data/raw/vannmiljo/Vm_Copper_2025.12.05.xlsx"),
    vm_sites_1 = here("data/raw/vannmiljo/Vm_Copper_Sites_2025.12.05-1.xlsx"),
    vm_sites_2 = here("data/raw/vannmiljo/Vm_Copper_Sites_2025.12.05-2.xlsx"),
    vm_sites_3 = here("data/raw/vannmiljo/Vm_Copper_Sites_2025.12.05-3.xlsx"),
    lookup_medium = here("data/clean/Vm_medium_lookup_matrix_filled.csv"),
    lookup_vannkategori = here("data/clean/vm_sites_codes_lookup.csv"),
    lookup_methods = here("data/clean/vm_methods_lookup_filled.csv"),
    methods_analysis = here(
      "data/raw/vannmiljo/Vannmiljø_Analysemetode_2025-12-15.xlsx"
    ),
    methods_sampling = here(
      "data/raw/vannmiljo/Vannmiljø_Prøvetakingsmetode_2025-12-15.xlsx"
    ),
    lookup_campaigns = here("data/clean/Vm_lookup_campaigns.csv"),
    lookup_units = here(
      "data/raw/vannmiljo/Vannmiljø_Enhet_2025-12-30.xlsx"
    ),
    lookup_species = here("data/clean/Vm_species_lookup.csv") # not actually used rn
  ),

  # Filter criteria
  # * is used as a wildcard in compartment lookups
  filters = list(
    date_start = as.Date("2010-01-01"),
    date_end = as.Date("2025-12-05"),
    exclude_sites = "Svalbard, ENSB-Kilde 2",
    compartment = c("Aquatic", "Biota", "*"),
    compartment_sub = c(
      "Freshwater",
      "Aquatic Sediment",
      "Marine/Salt Water",
      "Brackish/Transitional Water",
      "Biota, Aquatic",
      "*"
    )
  ),

  # Campaign metadata
  metadata = list(
    campaign_name = "Vannmiljø Copper Monitoring 2010-2025",
    campaign_short = "Vm_2010_2025",
    reference_id = "VannmiljøCopper2010-2025",
    entered_by = "Sam Welch",
    organisation = "Miljødirektoratet"
  )
)

```

```{r}
#| label: tbl-config-filters

config$filters |>
  enframe(name = "Parameter", value = "Value") |>
  kable()
```

```{r}
#| label: tbl-config-metadata

config$metadata |>
  enframe(name = "Parameter", value = "Value") |>
  kable()
```

## Helper Functions

```{r}
#| label: setup-helper-functions
# Convert Vannmiljø operator codes to eData flags
vm_convert_operator <- function(col) {
  case_match(
    col,
    "=" ~ "",
    "<" ~ "< LOQ",
    ">" ~ "What is going on here. Why higher than?",
    "ND" ~ "< LOD"
  )
}

vm_convert_unit <- function(col) {
  case_match(
    col,
    "µg/l" ~ "µg/L",
    "mg/kg t.v." ~ "mg/kg (dry)",
    "mg/kg v.v." ~ "mg/kg (wet)",
    .default = "UNIT CONVERSION ERROR"
  )
}
```

# Load Raw Data

## Measurements Data

Downloaded 2025.12.05, data for 2025.01.01 - 2025.12.05, all kommune, all media, all campaigns

Search for Kobber & Kobberpyrition: 138615 hits

NOTE: Data may extend beyond 2025 date range despite request parameters

```{r}
#| label: load-raw-measurements

# Downloaded 2025.12.05, data for 2025.01.01 - 2025.12.05, all kommune, all media, all campaigns
# Search for Kobber & Kobberpyrition: 138615 hits
# NOTE: Data may extend beyond 2025 date range despite request parameters

if (!exists("vm_raw_copper")) {
  vm_raw_copper <- read_excel(
    path = config$files$vm_copper,
    sheet = 1,
    guess_max = 138615 # Avoid automatic type conversion warnings
  )
}
```

`summarytools::dfSummary()` generates an initial summary of the dataset here. It's not perfect but it's one line of code, and generally very helpful.

```{r}
#| label: Initial data inspection
print(
  dfSummary(
    vm_raw_copper,
    varnumbers = FALSE,
    valid.col = FALSE,
    graph.magnif = 0.82
  ),
  method = 'render'
)
```

## Sites Data

NOTE: Vannmiljø export limit requires splitting site codes into multiple files

Procedure:

1.  Extract unique site codes: vm_raw_copper\$Vannlok_kode \|\> unique() \|\> writeClipboard()
2.  Replace newlines with commas: Ctrl+H find "\n" replace with ", "
3.  Split into chunks (Vannmiljø fails silently above unknown threshold)
4.  Download as separate Excel files

```{r}
#| label: load-raw-sites
#| echo: false

vm_raw_sites <- read_excel(
  config$files$vm_sites_1,
  guess_max = 10000
) |>
  add_row(read_excel(
    config$files$vm_sites_2,
    guess_max = 10000
  )) |>
  add_row(read_excel(
    config$files$vm_sites_3,
    guess_max = 10000
  ))

message(glue("Loaded {nrow(vm_raw_sites)} site records"))
```

## Lookup Tables

This part is very messy - we generated a bunch of lookup tables (semi-reproducibly) and filled them by hand/with Claude. I don't have a lot of confidence in any of them.

Some of the lookups are actually direct from Vannmiljø though - they're the xlsx ones. They're also bad, but for different reasons!

```{r}
#| label: load-lookups
#| echo: false

# Medium ID lookup
# Maps Vannmiljø Medium_id to eData compartment/species/tissue fields
vm_lookup_medium_raw <- read_csv(
  config$files$lookup_medium,
  guess_max = 1000000000, # this may not be the best idea in the world
  show_col_types = FALSE # TODO: Come back to this
)

vm_lookup_medium <- vm_lookup_medium_raw |>
  rename_with(
    ~ paste0(., "_medium"),
    c(
      ENVIRON_COMPARTMENT,
      ENVIRON_COMPARTMENT_SUB,
      SPECIES_GROUP,
      SAMPLE_SPECIES,
      SPECIES_GENDER,
      SAMPLE_TISSUE,
      SITE_GEOGRAPHIC_FEATURE,
      SITE_GEOGRAPHIC_FEATURE_SUB
    )
  )

# Vannkategori lookup
# Maps Vannmiljø Vannkategori codes to eData site/compartment fields
vm_lookup_vannkategori_raw <- read_csv(
  config$files$lookup_vannkategori,
  show_col_types = FALSE
)

vm_lookup_vannkategori <- vm_lookup_vannkategori_raw |>
  rename_with(
    ~ paste0(., "_vkat"),
    c(
      ENVIRON_COMPARTMENT,
      ENVIRON_COMPARTMENT_SUB,
      SITE_GEOGRAPHIC_FEATURE,
      SITE_GEOGRAPHIC_FEATURE_SUB
    )
  )

# Methods lookup
vm_lookup_analysis <- read_excel(config$files$methods_analysis)
vm_lookup_sampling <- read_excel(config$files$methods_sampling)
vm_lookup_methods <- read_csv(
  config$files$lookup_methods,
  show_col_types = FALSE
) |>
  mutate(
    CAMPAIGN_NAME = config$metadata$campaign_short,
    PROTOCOL_ID = generate_protocol_id(
      PROTOCOL_CATEGORY,
      PROTOCOL_NAME,
      1, # FIXME: We don't have unique numbers for this rn.
      config$metadata$campaign_short
    )
  )
vm_lookup_campaigns <- read_csv(
  config$files$lookup_campaigns,
  show_col_types = FALSE
)

vm_lookup_units <- read_excel(config$files$lookup_units)

vm_lookup_species <- read_csv(
  config$files$lookup_species,
  show_col_types = FALSE
)

message("Loaded 8 lookup tables")
```

# Data Processing

## Merge Datasets

We join sites and measurements by `"Vannlok_kode" = "Vannlokalitetskode"`, then our compartment/etc. lookups.

```{r}
#| label: processing-merge-datasets
#| echo: false

# Join sites to measurements
vm_merged <- vm_raw_copper |>
  left_join(
    vm_raw_sites,
    by = c("Vannlok_kode" = "Vannlokalitetskode")
  )

message(glue("Merged sites: {nrow(vm_merged)} rows"))

# Add lookup tables
vm_merged <- vm_merged |>
  left_join(
    vm_lookup_medium |>
      rename(MediumID_Name = Name, MediumID_Description = Description),
    by = c(Medium_id = "MediumID")
  ) |>
  left_join(
    vm_lookup_vannkategori |>
      rename(
        Vannkategori_Name = Name,
        Vannkategori_Description = Description
      ),
    by = c(Vannkategori = "VannkategoriID")
  ) |>
  left_join(
    vm_lookup_campaigns,
    by = c(Aktivitet_id = "Aktivitet_id")
  ) |>
  left_join(
    vm_lookup_units |> rename(Unit_Name = Name, Unit_Description = Description),
    by = c(Enhet_id = "UnitID")
  )

message(
  "Joined data to lookup tables: vm_lookup_medium, vm_lookup_vannkategori, vm_lookup_campaigns, vm_lookup_units"
)
```

## Initial Exploration

We don't yet have an oversight over the distribution of site types, biota, tissue, and media in the dataset. This will inform what and where we filter.

```{r}
#| label: fig-heatmap-campaign-vannkategory

# Create a summary of counts by the two variables
vm_summary <- vm_merged |>
  group_by(CAMPAIGN_DESCRIPTION_EN, Vannkategori_Description) |>
  summarise(n_samples = n(), .groups = "drop")

# Calculate text color threshold
threshold_value <- mean(log10(vm_summary$n_samples), na.rm = TRUE)

# Basic heatmap with ggplot2
vm_summary |>
  ggplot(aes(
    x = Vannkategori_Description,
    y = CAMPAIGN_DESCRIPTION_EN,
    fill = n_samples
  )) +
  geom_tile() +
  geom_text(
    aes(
      label = scales::comma(n_samples),
      color = if_else(log10(n_samples) > threshold_value, "black", "white")
    ),
    size = 3
  ) +
  scale_fill_viridis_c(trans = "log10") +
  scale_color_identity() +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 7))
```

```{r}
#| label: fig-heatmap-medium-campaign
# Create a summary of counts by the two variables
vm_summary_medium <- vm_merged |>
  group_by(CAMPAIGN_DESCRIPTION_EN, MediumID_Name) |>
  summarise(n_samples = n(), .groups = "drop")

# Calculate text color threshold
threshold_value_medium <- mean(log10(vm_summary_medium$n_samples), na.rm = TRUE)

# Calculate total samples per campaign for ordering
campaign_order <- vm_summary_medium |>
  group_by(CAMPAIGN_DESCRIPTION_EN) |>
  summarise(total = sum(n_samples)) |>
  arrange(total) |>
  pull(CAMPAIGN_DESCRIPTION_EN)

# Basic heatmap with ggplot2
vm_summary_medium |>
  mutate(
    CAMPAIGN_DESCRIPTION_EN = factor(
      CAMPAIGN_DESCRIPTION_EN,
      levels = campaign_order
    )
  ) |>
  ggplot(aes(
    x = MediumID_Name,
    y = CAMPAIGN_DESCRIPTION_EN,
    fill = n_samples
  )) +
  geom_tile() +
  geom_text(
    aes(
      label = scales::comma(n_samples),
      color = if_else(
        log10(n_samples) > threshold_value_medium,
        "black",
        "white"
      )
    ),
    size = 3
  ) +
  scale_fill_viridis_c(trans = "log10") +
  scale_color_identity() +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 7))
```

```{r}
#| label: fig-heatmap-vannkategori-medium

# Create a summary of counts by the two variables
vm_summary_vkat_medium <- vm_merged |>
  group_by(Vannkategori_Description, MediumID_Name) |>
  summarise(n_samples = n(), .groups = "drop")

# Calculate text color threshold
threshold_value_vkat_medium <- mean(
  log10(vm_summary_vkat_medium$n_samples),
  na.rm = TRUE
)

# Basic heatmap with ggplot2
vm_summary_vkat_medium |>
  ggplot(aes(
    x = MediumID_Name,
    y = Vannkategori_Description,
    fill = n_samples
  )) +
  geom_tile() +
  geom_text(
    aes(
      label = scales::comma(n_samples),
      color = if_else(
        log10(n_samples) > threshold_value_vkat_medium,
        "black",
        "white"
      )
    ),
    size = 3
  ) +
  scale_fill_viridis_c(trans = "log10") +
  scale_color_identity() +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 7))
```

```{r}
#| label: tbl-sample-distribution
#| tbl-cap: "Distribution of samples across water categories and media types"

# Calculate sample distribution with cumulative percentages
sample_distribution <- vm_summary_vkat_medium |>
  arrange(desc(n_samples)) |>
  mutate(
    pct_of_total = 100 * n_samples / sum(n_samples),
    cumulative_n = cumsum(n_samples),
    cumulative_pct = 100 * cumulative_n / sum(n_samples)
  ) |>
  select(
    Vannkategori_Description,
    MediumID_Name,
    n_samples,
    pct_of_total,
    cumulative_n,
    cumulative_pct
  )

datatable(
  sample_distribution,
  colnames = c(
    "Water Category",
    "Medium",
    "N Samples",
    "% of Total",
    "Cumulative N",
    "Cumulative %"
  ),
  options = list(
    pageLength = 10,
    order = list(list(3, 'desc'))
  )
) |>
  formatRound(columns = c("pct_of_total", "cumulative_pct"), digits = 1) |>
  formatCurrency(
    columns = c("n_samples", "cumulative_n"),
    currency = "",
    digits = 0
  )
```

@tbl-sample-distribution shows the distribution of samples across Vannkategori and MediumID. This is what I'm using as a guide of where to spend my time. If I can capture 90% of the samples (before the other filtration steps) then I'll call it done.

@fig-sample-distribution-biota is biota. Some of these are certainly irrelevant - e.g. *Felis catus*... but how to filter?

```{r}
#| label: fig-sample-distribution-biota

# Create a summary of counts by the two variables (filter to non-NA species)
vm_summary_species <- vm_merged |>
  filter(!is.na(VitenskapligNavn), VitenskapligNavn != "Artsuavhengig") |>
  group_by(CAMPAIGN_DESCRIPTION_EN, VitenskapligNavn) |>
  summarise(n_samples = n(), .groups = "drop")

# Calculate text color threshold
threshold_value_species <- mean(
  log10(vm_summary_species$n_samples),
  na.rm = TRUE
)

# Calculate total samples per campaign for ordering (y-axis)
campaign_order_species <- vm_summary_species |>
  group_by(CAMPAIGN_DESCRIPTION_EN) |>
  summarise(total = sum(n_samples)) |>
  arrange(total) |>
  pull(CAMPAIGN_DESCRIPTION_EN)

# Calculate total samples per species for ordering (x-axis)
species_order <- vm_summary_species |>
  group_by(VitenskapligNavn) |>
  summarise(total = sum(n_samples)) |>
  arrange(total) |>
  pull(VitenskapligNavn)

# Basic heatmap with ggplot2
vm_summary_species |>
  mutate(
    CAMPAIGN_DESCRIPTION_EN = factor(
      CAMPAIGN_DESCRIPTION_EN,
      levels = campaign_order_species
    ),
    VitenskapligNavn = factor(VitenskapligNavn, levels = species_order)
  ) |>
  ggplot(aes(
    x = VitenskapligNavn,
    y = CAMPAIGN_DESCRIPTION_EN,
    fill = n_samples
  )) +
  geom_tile() +
  geom_text(
    aes(
      label = scales::comma(n_samples),
      color = if_else(
        log10(n_samples) > threshold_value_species,
        "black",
        "white"
      )
    ),
    size = 3
  ) +
  scale_fill_viridis_c(trans = "log10") +
  scale_color_identity() +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 7))
```

Finally, what about units @fig-heatmap-units-medium? Units are, I'm relieved to say, sensible.

```{r}
#| label: fig-heatmap-units-medium

# Create a summary of counts by the two variables
vm_summary_units_medium <- vm_merged |>
  group_by(Unit_Name, MediumID_Name) |>
  summarise(n_samples = n(), .groups = "drop")

# Calculate text color threshold
threshold_value_units_medium <- mean(
  log10(vm_summary_units_medium$n_samples),
  na.rm = TRUE
)

# Basic heatmap with ggplot2
vm_summary_units_medium |>
  ggplot(aes(
    x = MediumID_Name,
    y = Unit_Name,
    fill = n_samples
  )) +
  geom_tile() +
  geom_text(
    aes(
      label = scales::comma(n_samples),
      color = if_else(
        log10(n_samples) > threshold_value_units_medium,
        "black",
        "white"
      )
    ),
    size = 3
  ) +
  scale_fill_viridis_c(trans = "log10") +
  scale_color_identity() +
  theme_minimal() +
  coord_flip()
```

## Apply Filters

As above, we filter data very heavily here, so we don't have to spend a lot of time on edge cases.

Data Quality Filters Applied: 2025-12-16

Included:

-   See config tables in \[Configuration\](##Configuration) for specifics
-   Point sites only (no polygons)
-   Mainland Norway only (excluded Svalbard)

Excluded:

-   Svalbard sites (n=1)

```{r}
#| label: processing-apply-filters
#| echo: false

vm_filtered <- vm_merged |>
  # Add sampling date as Date type
  mutate(SAMPLING_DATE = as.Date(Tid_provetak)) |>
  # Apply compartment filters
  filter(
    ENVIRON_COMPARTMENT_vkat %in%
      config$filters$compartment |
      ENVIRON_COMPARTMENT_medium %in% config$filters$compartment,
    ENVIRON_COMPARTMENT_SUB_vkat %in%
      config$filters$compartment_sub |
      ENVIRON_COMPARTMENT_SUB_medium %in% config$filters$compartment_sub
  ) |>
  # Apply site type filters
  filter(
    Objekttype == "point",
    Vannlokalitetsnavn != config$filters$exclude_sites
  ) |>
  # Apply date filter
  filter(SAMPLING_DATE >= config$filters$date_start)

message(glue(
  "After filtering: {nrow(vm_filtered)} rows ({round(100*nrow(vm_filtered)/nrow(vm_merged), 1)}% of merged data)"
))

```

Let's also take a look at what we're excluding (based on compartments)

```{r}
#| label: processing-excluded-data
#| echo: false

vm_excluded <- vm_merged |>
  # Add sampling date as Date type
  mutate(SAMPLING_DATE = as.Date(Tid_provetak)) |>
  # Apply compartment filters
  filter(
    !(ENVIRON_COMPARTMENT_vkat %in% config$filters$compartment),
    !(ENVIRON_COMPARTMENT_medium %in% config$filters$compartment),
    !(ENVIRON_COMPARTMENT_SUB_vkat %in% config$filters$compartment_sub),
    !(ENVIRON_COMPARTMENT_SUB_medium %in% config$filters$compartment_sub)
  )

count(vm_excluded, ENVIRON_COMPARTMENT_SUB_vkat)
count(vm_excluded, ENVIRON_COMPARTMENT_medium)
count(vm_merged, Objekttype)

message(glue(
  "Excluded: {nrow(vm_excluded)} rows ({round(100*nrow(vm_excluded)/nrow(vm_merged), 1)}% of merged data)"
))

```

A Sankey diagram would be better, but @fig-filter-visualisation will do for now.

```{r}
#| label: fig-filter-visualisation

# Create filtering summary
filter_flow <- tibble(
  stage = c("Raw", "Compartments", "Point sites", "Date range"),
  records = c(
    nrow(vm_merged),
    vm_merged |>
      filter(
        ENVIRON_COMPARTMENT_SUB_vkat %in%
          config$filters$compartment_sub |
          ENVIRON_COMPARTMENT_SUB_medium %in% config$filters$compartment_sub
      ) |>
      nrow(),
    vm_merged |>
      filter(
        ENVIRON_COMPARTMENT_SUB_vkat %in%
          config$filters$compartment_sub |
          ENVIRON_COMPARTMENT_SUB_medium %in% config$filters$compartment_sub,
        Objekttype == "point"
      ) |>
      nrow(),
    nrow(vm_filtered)
  )
) |>
  mutate(
    pct = round(100 * records / first(records), 1),
    label = glue("{scales::comma(records)}\n({pct}%)")
  )

# Simple bar plot
ggplot(filter_flow, aes(x = stage, y = records)) +
  geom_col(aes(fill = stage)) +
  geom_text(aes(label = label), vjust = -0.5, size = 3) +
  scale_fill_discrete(name = "Filtering Stage") +
  scale_x_discrete(limits = filter_flow$stage) +
  scale_y_continuous(
    labels = scales::comma,
    expand = expansion(mult = c(0, 0.1))
  ) +
  labs(
    title = "Data Retention Through Filtering Pipeline",
    x = NULL,
    y = "Number of Records"
  ) +
  theme_minimal()
```

At time of writing, we've got 94% of compartment data, which is well above our goal of 90%.

## Quality Checks

We check for conflicts between our two lookups here - because we look up compartments, sites, biota, etc. based on both site `Vannlokalkode` and measurments' `MediumID`, it's possible that we might end up with conflicts and/or cases where neither of these give us the relevant data and we have to guess from context. Right now we filter down to only freshwater cases, hence the fact that we don't get hits below.

- [ ] We have plenty of conflicts now! Hooray, and also... damn.

Note: It's not necessarily a problem that something turns up as a conflict - we fix a lot of these in the next steps.

```{r}
#| label: tbl-compartment-conflicts
#| tbl-cap: "Conflicts between vannkategori and medium lookups for environmental compartments"

# Check for conflicts between lookup sources
conflict_check <- vm_filtered |>
  group_by(
    ENVIRON_COMPARTMENT_vkat,
    ENVIRON_COMPARTMENT_medium,
    ENVIRON_COMPARTMENT_SUB_vkat,
    ENVIRON_COMPARTMENT_SUB_medium
  ) |>
  reframe(count = n()) |>
  mutate(
    compartment_conflict = ENVIRON_COMPARTMENT_vkat !=
      ENVIRON_COMPARTMENT_medium,
    subcompartment_conflict = ENVIRON_COMPARTMENT_SUB_vkat !=
      ENVIRON_COMPARTMENT_SUB_medium,
    Conflict = case_when(
      compartment_conflict & subcompartment_conflict ~ "Main, Sub",
      compartment_conflict ~ "Main",
      subcompartment_conflict ~ "Sub",
      .default = NA_character_
    )
  ) |>
  filter(!is.na(Conflict)) |>
  arrange(desc(count))

if (nrow(conflict_check) > 0) {
  warning("Conflicts detected between vannkategori and medium lookups")
}

datatable(
  conflict_check |>
    select(
      ENVIRON_COMPARTMENT_vkat,
      ENVIRON_COMPARTMENT_medium,
      ENVIRON_COMPARTMENT_SUB_vkat,
      ENVIRON_COMPARTMENT_SUB_medium,
      count,
      Conflict
    ),
  colnames = c(
    "Compartment (Vannkategory)",
    "Compartment (MediumName)",
    "Subcompartment (Vannkategory)",
    "Subcompartment (MediumName)",
    "Rows",
    "Conflict Type"
  )
)
```

@tbl-compartment-conflicts lists all combinations of compartments and subcompartments, and whether any conflict is detected between the two compartments. I feel a need to be explicit and reproducible about what trumps what where, so see below.

-   Subcompartment (Vkat) == Freshwater & Subcompartment (Medium) == Aquatic Sediment -\> Aquatic Sediment
-  If one column returns biota, we use that (even if it's a fish e.g. living in the sea...)

```{r}
#| label: resolve-compartment-conflicts

# use the below function to resolve specific conflicts
resolve_compartment_conflicts <- function(df) {
  df <- df |>
    mutate(
      ENVIRON_COMPARTMENT_resolved = case_when(
        # Explicit rule: Biota trumps all
        ENVIRON_COMPARTMENT_vkat == "Biota" |
          ENVIRON_COMPARTMENT_medium == "Biota" ~ "Biota",

        # No conflict, return NA
        .default = NA_character_
      ),
      ENVIRON_COMPARTMENT_SUB_resolved = case_when(
        # Explicit rule: Biota trumps all
        ENVIRON_COMPARTMENT_SUB_vkat == "Biota, Aquatic" |
          ENVIRON_COMPARTMENT_SUB_medium == "Biota, Aquatic" ~ "Biota, Aquatic",

        # Explicit rule: Medium trumps Vkat for sediment
        ENVIRON_COMPARTMENT_SUB_vkat == "Freshwater" &
          ENVIRON_COMPARTMENT_SUB_medium == "Aquatic Sediment" ~
          "Aquatic Sediment",

        # No conflict, return NA
        .default = NA_character_
      )
    )

  n_resolved_sub <- df |> tally(!is.na(ENVIRON_COMPARTMENT_SUB_resolved))
  n_resolved <- df |> tally(!is.na(ENVIRON_COMPARTMENT_resolved))

  message(glue(
    "Resolved {n_resolved} conflicting compartments and {n_resolved_sub} conflicting subcompartments."
  ))

  df
}

vm_compartment_conflicts_resolved <- resolve_compartment_conflicts(
  vm_filtered
) |>
  mutate(
    ENVIRON_COMPARTMENT_SUB = case_when(
      !is.na(
        ENVIRON_COMPARTMENT_SUB_resolved
      ) ~ ENVIRON_COMPARTMENT_SUB_resolved,
      .default = ENVIRON_COMPARTMENT_SUB_medium
    ),
    ENVIRON_COMPARTMENT = case_when(
      !is.na(ENVIRON_COMPARTMENT_resolved) ~ ENVIRON_COMPARTMENT_resolved,
      .default = ENVIRON_COMPARTMENT_medium
    )
  )
```

@tbl-geographic-conflicts also lists all combinations, which means it also lists things that actually aren't problems. Again, may need cleaning up.

```{r}
#| label: tbl-geographic-conflicts
#| tbl-cap: "Conflicts between vannkategori and medium lookups for geographic features"

# Check geographic feature consistency
geo_check <- vm_compartment_conflicts_resolved |>
  group_by(
    SITE_GEOGRAPHIC_FEATURE_vkat,
    SITE_GEOGRAPHIC_FEATURE_medium,
    SITE_GEOGRAPHIC_FEATURE_SUB_vkat,
    SITE_GEOGRAPHIC_FEATURE_SUB_medium
  ) |>
  reframe(count = n()) |>
  mutate(
    # Only flag as conflict if both values are present and different
    geo_conflict = !has_data_issue(SITE_GEOGRAPHIC_FEATURE_vkat) &
      !has_data_issue(SITE_GEOGRAPHIC_FEATURE_medium) &
      SITE_GEOGRAPHIC_FEATURE_vkat != "*" &
      SITE_GEOGRAPHIC_FEATURE_medium != "*" &
      SITE_GEOGRAPHIC_FEATURE_vkat != SITE_GEOGRAPHIC_FEATURE_medium,

    geo_sub_conflict = !has_data_issue(SITE_GEOGRAPHIC_FEATURE_SUB_vkat) &
      !has_data_issue(SITE_GEOGRAPHIC_FEATURE_SUB_medium) &
      SITE_GEOGRAPHIC_FEATURE_SUB_vkat != "*" &
      SITE_GEOGRAPHIC_FEATURE_SUB_medium != "*" &
      SITE_GEOGRAPHIC_FEATURE_SUB_vkat != SITE_GEOGRAPHIC_FEATURE_SUB_medium,

    Conflict = case_when(
      geo_conflict & geo_sub_conflict ~ "Main, Sub",
      geo_conflict ~ "Main",
      geo_sub_conflict ~ "Sub",
      .default = NA_character_
    )
  ) |>
  filter(!is.na(Conflict)) |>
  arrange(desc(count))

if (nrow(geo_check) > 0) {
  warning(glue(
    "Geographic feature conflicts: {nrow(geo_check)} combinations"
  ))
}

datatable(
  geo_check |>
    select(
      SITE_GEOGRAPHIC_FEATURE_vkat,
      SITE_GEOGRAPHIC_FEATURE_medium,
      SITE_GEOGRAPHIC_FEATURE_SUB_vkat,
      SITE_GEOGRAPHIC_FEATURE_SUB_medium,
      count,
      Conflict
    ),
  colnames = c(
    "Feature (Vannkategory)",
    "Feature (MediumName)",
    "Subfeature (Vannkategory)",
    "Subfeature (MediumName)",
    "Rows",
    "Conflict Type"
  )
)
```

Since we haven't actually had any geographical conflicts flagged yet there's no need to write code to defuse them. But watch this space.

```{r}
#| label: resolve-feature-conflicts

```

# Create eData Tables

## Campaign

```{r}
#| label: edata-campaign

edata_campaign <- initialise_campaign_tibble() |>
  add_row(
    CAMPAIGN_NAME_SHORT = config$metadata$campaign_short,
    CAMPAIGN_NAME = config$metadata$campaign_name,
    CAMPAIGN_START_DATE = config$filters$date_start,
    CAMPAIGN_END_DATE = config$filters$date_end,
    RELIABILITY_SCORE = NA_character_,
    RELIABILITY_EVAL_SYS = NA_character_,
    CONFIDENTIALITY_EXPIRY_DATE = as.Date(NA),
    ORGANISATION = config$metadata$organisation,
    ENTERED_BY = config$metadata$entered_by,
    ENTERED_DATE = as.Date(Sys.Date()),
    CAMPAIGN_COMMENT = "Copper and copper pyrithione measurements from Vannmiljø database covering all Norwegian municipalities and media types"
  )

message("Created campaign table")
```

## Reference

```{r}
#| label: edata-reference

edata_reference <- initialise_references_tibble() |>
  add_row(
    REFERENCE_ID = config$metadata$reference_id,
    REFERENCE_TYPE = "Database",
    DATA_SOURCE = "Vannmiljø",
    AUTHOR = config$metadata$organisation,
    TITLE = "Vannmiljø Database - Copper and Copper Pyrithione Data",
    YEAR = 2025L,
    ACCESS_DATE = config$filters$date_end,
    PERIODICAL_JOURNAL = NA_character_,
    VOLUME = NA_integer_,
    ISSUE = NA_integer_,
    PUBLISHER = NA_character_,
    INSTITUTION = config$metadata$organisation,
    DOI = NA_character_,
    URL = "https://www.vannmiljo.no/",
    ISBN_ISSN = NA_character_,
    EDITION = NA_character_,
    DOCUMENT_NUMBER = NA_character_,
    REF_COMMENT = glue(
      "Downloaded {config$filters$date_end}, data for {config$filters$date_start} to {config$filters$date_end}, ",
      "all kommune, all media, all campaigns. Search for Kobber & Kobberpyrition. 138615 hits."
    )
  )

message("Created reference table")
```

## Parameters

```{r}
#| label: edata-parameters

# No copper pyrithione reported for 2025, only copper
edata_parameters <- initialise_parameters_tibble() |>
  add_row(
    PARAMETER_TYPE = NA_character_,
    PARAMETER_TYPE_SUB = NA_character_,
    MEASURED_TYPE = NA_character_,
    PARAMETER_NAME = "Copper",
    PARAMETER_NAME_SUB = NA_character_,
    INCHIKEY_SD = NA_character_,
    PUBCHEM_CID = NA_integer_,
    CAS_RN = NA_character_,
    ENTERED_BY = glue("{config$metadata$entered_by} from Vannmiljø"),
    PARAMETER_COMMENT = NA_character_
  )

message("Created parameters table")
```

## Sites

```{r}
#| label: edata-sites

# Extract unique sites with relevant metadata
vm_sites_unique <- vm_filtered |>
  select(
    Vannlok_kode,
    Vannlokalitetsnavn,
    Beskrivelse,
    `UTM33 Ost (X)`,
    `UTM33 Nord (Y)`,
    `Knytt til påvirkning`,
    SITE_GEOGRAPHIC_FEATURE_medium,
    SITE_GEOGRAPHIC_FEATURE_vkat,
    SITE_GEOGRAPHIC_FEATURE_SUB_medium,
    SITE_GEOGRAPHIC_FEATURE_SUB_vkat
  ) |>
  distinct() |>
  # Clean up emission source
  mutate(
    Emission_Source = case_match(
      `Knytt til påvirkning`,
      c("Industri", "INDUSTRI") ~ "Industrial",
      c("Akvakultur", "AKVAKULTUR") ~ "Aquaculture",
      .default = NA_character_
    )
  )

message(glue("Extracted {nrow(vm_sites_unique)} unique sites"))

# Format to eData structure
edata_sites_temp <- vm_sites_unique |>
  mutate(
    SITE_CODE = glue("Vannmiljø_{Vannlok_kode}"),
    SITE_NAME = glue("Vannmiljø Station {Vannlokalitetsnavn}"),
    SITE_GEOGRAPHIC_FEATURE = SITE_GEOGRAPHIC_FEATURE_vkat,
    SITE_GEOGRAPHIC_FEATURE_SUB = SITE_GEOGRAPHIC_FEATURE_SUB_medium,
    COUNTRY_ISO = "Norway",
    OCEAN_IHO = "Not relevant",
    ENTERED_BY = glue("{config$metadata$entered_by} (Vm Conversion)"),
    ENTERED_DATE = as.character(today()),
    ALTITUDE_VALUE = 0, # Altitude not relevant for water monitoring
    ALTITUDE_UNIT = "m",
    # Combine description and emission source where available
    SITE_COMMENT = case_when(
      !is.na(Beskrivelse) &
        Beskrivelse != "" &
        !is.na(Emission_Source) &
        Emission_Source != "" ~
        glue(
          "Vm Original Comment: {Beskrivelse}. Vm Emission Source: {Emission_Source}"
        ),
      !is.na(Beskrivelse) & Beskrivelse != "" ~
        glue("Vm Original Comment: {Beskrivelse}"),
      !is.na(Emission_Source) & Emission_Source != "" ~
        glue("Vm Emission Source: {Emission_Source}"),
      .default = NA_character_
    )
  )

# Reproject coordinates from UTM33 to WGS84
sites_sf <- edata_sites_temp |>
  st_as_sf(coords = c("UTM33 Ost (X)", "UTM33 Nord (Y)"), crs = 25833) |>
  st_transform(4326) |>
  mutate(
    LATITUDE = st_coordinates(geometry)[, 2],
    LONGITUDE = st_coordinates(geometry)[, 1],
    SITE_COORDINATE_SYSTEM = "WGS84"
  )

sites_coords <- sites_sf |>
  st_coordinates() |>
  as.data.frame() |>
  bind_cols(st_drop_geometry(sites_sf))

# Finalize sites table
edata_sites <- edata_sites_temp |>
  left_join(
    sites_sf |>
      st_drop_geometry() |>
      select(SITE_CODE, LATITUDE, LONGITUDE, SITE_COORDINATE_SYSTEM),
    by = "SITE_CODE"
  ) |>
  select(
    SITE_CODE,
    SITE_NAME,
    SITE_GEOGRAPHIC_FEATURE,
    SITE_GEOGRAPHIC_FEATURE_SUB,
    LATITUDE,
    LONGITUDE,
    SITE_COORDINATE_SYSTEM,
    COUNTRY_ISO,
    OCEAN_IHO,
    ENTERED_BY,
    ENTERED_DATE,
    SITE_COMMENT
  )

# Validate against eData schema
edata_sites <- initialise_sites_tibble() |> add_row(edata_sites)

message(glue("Created sites table: {nrow(edata_sites)} sites"))
```

Visually inspecting transformed site distribution allows us to check that a) sites are roughly where they should be and b) the correct CRS transformation was used.

```{r}
#| label: fig-sanity-map

# Sanity check: plot sites on map
world_map <- map_data("world")
ggplot() +
  geom_polygon(
    data = world_map,
    aes(x = long, y = lat, group = group),
    fill = "lightgray",
    colour = "white"
  ) +
  geom_hex(
    data = sites_coords,
    aes(x = X, y = Y),
    bins = 1000
  ) +
  scale_fill_viridis_c() +
  labs(
    title = "Vannmiljø Freshwater Sites 2025",
    subtitle = "Binned into 1000 hexagons, coloured by number of sites in area, for faster rendering."
  )

```

## Methods

```{r}
#| label: edata-methods

# Methods Mapping
# NOTE: Methods mapping simplified for initial conversion
# Current approach:
# - Simple 1:1 mapping from Vannmiljø codes
# - Filtered = assumed 0.45 µm
# - Total concentration only (no fractionation detail)
#
# Known limitations:
# - "UKJENT" (Unknown) methods not properly handled
# - Fractionation protocol oversimplified
# - Extraction protocols not captured
# - Some method details may be in Norwegian standards not reviewed

# Filter methods to those actually used in freshwater data
vm_methods_used_analysis <- vm_lookup_analysis |>
  filter(AnalysisMethodID %in% vm_filtered$Analysemetode_id)

vm_methods_used_sampling <- vm_lookup_sampling |>
  filter(
    SamplingMethodID %in% vm_filtered$Provetakmetode_id,
    str_detect(Name, "erskvann"), # Freshwater only
    !str_detect(Name, "sediment") # Exclude sediment
  )

# Merge and categorize
edata_methods <- vm_methods_used_sampling #
# TODO: Renable methods.
# bind_rows(vm_methods_used_analysis) |>
# mutate(
#   ID = case_when(
#     !is.na(AnalysisMethodID) ~ AnalysisMethodID,
#     !is.na(SamplingMethodID) ~ SamplingMethodID,
#     .default = NA_character_
#   ),
#   Category = case_when(
#     !is.na(AnalysisMethodID) ~ "Analysis",
#     !is.na(SamplingMethodID) ~ "Sampling",
#     .default = NA_character_
#   ),
#   .keep = "unused"
# ) |>
# # Join with eData mapping
# left_join(
#   vm_lookup_methods,
#   by = c("PROTOCOL_ID", "Category")
# ) |>
# group_by(PROTOCOL_CATEGORY) |>
# mutate(
#   CAMPAIGN_NAME = config$metadata$campaign_name,
#   PROTOCOL_ID = glue("{substr(PROTOCOL_CATEGORY, 1, 1)}{row_number()}-Vm2025")
# ) |>
# ungroup()

message(glue("Created methods table: {nrow(edata_methods)} protocols"))
```

## Samples

In the eData workflow, we generate a samples table by combining sites, parameters, dates and compartments. This isn't how we work with Vannmiljø, but it's useful to have it anyway because we need it to generate the biota table - which stores species, tissue, etc. information.

```{r}
#| label: edata-samples

# this function is copied from STOPeData::mod_samples_fct.R
# I guess we forgot to export it. Good candidate to be moved to eDataDRF.
# TODO: It's also quite slow, so we could probably make it better.
#' Generate Sample ID with Components ----
#' @param site_code Site code (vectorized)
#' @param parameter_name Parameter name (vectorized)
#' @param environ_compartment Environmental compartment (vectorized)
#' @param environ_compartment_sub Environmental sub-compartment (vectorized)
#' @param date Sampling date (vectorized)
#' @param subsample subsample
#' @importFrom stringr str_to_title str_remove_all
#' @import eDataDRF
#' @noRd
generate_sample_id_with_components <- function(
  site_code,
  parameter_name,
  environ_compartment,
  environ_compartment_sub,
  date,
  subsample = 1
) {
  # Create abbreviated versions for ID (vectorized)
  param_abbrev <- substr(gsub("[^A-Za-z0-9]", "", parameter_name), 1, 8)
  comp_abbrev <- substr(
    gsub("[^A-Za-z0-9]", "", environ_compartment_sub),
    1,
    12
  )
  date_abbrev <- gsub("-", "-", date)

  base_id <- glue("{site_code}-{param_abbrev}-{comp_abbrev}-{date_abbrev}")

  # Vectorized replicate
  # Subsamples will generally be text, so let's abbreviate them a bit

  subsample_suffix <- sapply(
    subsample,
    function(x) abbreviate_string(string = x, n_words = 3, case = "title")
  )
  paste0(base_id, "-R-", subsample_suffix)
}


# Create samples table

# TODO: Update to use geo conflicts resolved thingy as appropriate
edata_samples_wide <- vm_compartment_conflicts_resolved |>
  mutate(
    # Core identifiers
    SITE_CODE = glue("Vannmiljø_{Vannlok_kode}"),
    SITE_NAME = glue("Vannmiljø Station {Vannlokalitetsnavn}"),
    PARAMETER_NAME = "Copper",
    PARAMETER_TYPE = "Not reported",

    # Compartment information
    ENVIRON_COMPARTMENT = ENVIRON_COMPARTMENT,
    ENVIRON_COMPARTMENT_SUB = ENVIRON_COMPARTMENT_SUB,
    MEASURED_CATEGORY = NA_character_,

    SAMPLING_DATE = as.character(SAMPLING_DATE),

    # Sample information
    SUBSAMPLE = "NA",
    SAMPLE_ID = generate_sample_id_with_components(
      SITE_CODE,
      PARAMETER_NAME,
      ENVIRON_COMPARTMENT,
      ENVIRON_COMPARTMENT_SUB,
      SAMPLING_DATE,
      SUBSAMPLE
    )
  )

# Validate against eData schema
edata_samples <- initialise_samples_tibble() |>
  add_row(
    edata_samples_wide |>
      select(any_of(names(initialise_samples_tibble())))
  )


message(glue(
  "Created samples table: {nrow(edata_samples)} samples"
))
```

## Biota

Biota stuff. Some genders are implicit from the tissue type reported (e.g. gonads), but I don't think I want to worry about that here. Our assumptions:

- All species reported in Vannmiljø are correct
- No attempt was made to infer gender or lifestage

Nevertheless, we need to do a bit of extra work with scientific names, species groups, and tissue before proceeding. Currently:

- Laksesmolt needs to be recoded as Juvenile Salmo salmar

That's it.

```{r}
#| label: tbl-raw-biota

# Create biota table

# TODO: Update to use geo conflicts resolved thingy as appropriate
edata_biota <- vm_compartment_conflicts_resolved |>
  filter(ENVIRON_COMPARTMENT == "Biota")
message(glue(
  "{nrow(edata_biota)} biota samples found"
))

biota_frequency <- count(edata_biota, VitenskapligNavn) |>
  arrange(desc(n)) |>
  left_join(
    select(vm_lookup_species, VitenskapligNavn, species_group),
    by = "VitenskapligNavn"
  ) |>
  mutate(
    SAMPLE_SPECIES = case_when(
      VitenskapligNavn == "Laksesmolt" ~ "Salmo salar",
      TRUE ~ VitenskapligNavn
    ),
    SAMPLE_SPECIES_LIFESTAGE = case_when(
      VitenskapligNavn == "Laksesmolt" ~ "Juvenile",
      TRUE ~ "Not reported"
    )
  )

# Report data quality issues
n_missing_species_group <- biota_frequency |>
  filter(is.na(species_group)) |>
  pull(n) |>
  sum()


message(glue(
  "Biota data quality: {n_missing_species_group} samples with missing species group"
))


datatable(biota_frequency)

# there's certainly a more elegant way to do this than twice
edata_biota_merged <- edata_biota |>
  left_join(
    select(vm_lookup_species, VitenskapligNavn, species_group),
    by = "VitenskapligNavn"
  ) |>
  mutate(
    SAMPLE_SPECIES = case_when(
      VitenskapligNavn == "Laksesmolt" ~ "Salmo salar",
      TRUE ~ VitenskapligNavn
    ),
    SAMPLE_SPECIES_LIFESTAGE = case_when(
      VitenskapligNavn == "Laksesmolt" ~ "Juvenile",
      TRUE ~ "Not reported"
    )
  )
```

We also need to convert tissue types (stored as MediumID in Vannmiljø) into our appropriate equivalent. At time of writing there are only 3, which keeps life relatively easy.

```{r}
#| label: tbl-raw-tissue

# Create tissue table

# TODO: There are a lot more tissues to fix now!

# TODO: Update to use geo conflicts resolved thingy as appropriate
tissue_frequency <- count(edata_biota, MediumID_Name) |>
  arrange(desc(n)) |>
  mutate(
    SAMPLE_TISSUE = case_match(
      MediumID_Name,
      "Biota bløtdeler" ~ "Total soft tissues",
      "Biota gjeller" ~ "Gills",
      "Biota helkropp" ~ "Whole body",
      .default = "Unknown Tissue"
    )
  )

# Report data quality issues
n_unknown_tissue <- tissue_frequency |>
  filter(SAMPLE_TISSUE == "Unknown Tissue") |>
  pull(n) |>
  sum()

message(glue(
  "Biota data quality: {n_unknown_tissue} samples with unknown tissue type"
))

datatable(tissue_frequency)

edata_biota_tissue_merged <- edata_biota_merged |>
  mutate(
    SAMPLE_TISSUE = case_match(
      MediumID_Name,
      "Biota bløtdeler" ~ "Total soft tissues",
      "Biota gjeller" ~ "Gills",
      "Biota helkropp" ~ "Whole body",
      .default = "Unknown Tissue"
    )
  )

```


```{r}
#| label: eData-biota

vm_filtered

vm_samples_biota_only <- edata_biota_tissue_merged |>
  mutate(
    # Core identifiers
    SITE_CODE = glue("Vannmiljø_{Vannlok_kode}"),
    # SITE_NAME = glue("Vannmiljø Station {Vannlokalitetsnavn}"), added later
    PARAMETER_NAME = "Copper",
    # PARAMETER_TYPE = "Not reported", added later

    # Compartment information
    ENVIRON_COMPARTMENT = ENVIRON_COMPARTMENT,
    ENVIRON_COMPARTMENT_SUB = ENVIRON_COMPARTMENT_SUB,
    MEASURED_CATEGORY = NA_character_,

    SAMPLING_DATE = as.character(SAMPLING_DATE),

    # Sample information
    SUBSAMPLE = "NA",
    SAMPLE_ID = generate_sample_id_with_components(
      SITE_CODE,
      PARAMETER_NAME,
      ENVIRON_COMPARTMENT,
      ENVIRON_COMPARTMENT_SUB,
      SAMPLING_DATE,
      SUBSAMPLE
    ),

    # biota stuff
    SPECIES_GROUP = species_group,
    SAMPLE_SPECIES,
    SAMPLE_TISSUE,
    SAMPLE_SPECIES_LIFESTAGE,
    SAMPLE_SPECIES_GENDER = "Not reported",
    BIOTA_COMMENT = "Raw species names from Vannmiljø. May be erroneous.",
  )

# Validate against eData schema
edata_biota <- initialise_biota_tibble() |>
  add_row(
    vm_samples_biota_only |>
      select(any_of(names(initialise_biota_tibble())))
  )

message(glue(
  "Created biota table: {nrow(edata_biota)} rows"
))
```

Once we've handled biota separately we can merge them with the rest of the samples to create the intermediate table used to create the measurements table. This is a slightly strange way of doing things, but it works.

```{r}
#| label: merge-biota-samples

biota_samples_merged <- edata_samples_wide |> bind_rows(vm_samples_biota_only)

```

## Measurements

```{r}
#| label: edata-measurements

# Create measurements table

# TODO: Update to use geo conflicts resolved thingy as appropriate
edata_measurements <- biota_samples_merged |>
  mutate(
    # Core identifiers
    SITE_CODE,
    PARAMETER_NAME,
    SAMPLING_DATE = as.character(SAMPLING_DATE),
    CAMPAIGN_NAME_SHORT = config$metadata$campaign_short,
    REFERENCE_ID = config$metadata$reference_id,

    # Compartment information
    ENVIRON_COMPARTMENT,
    ENVIRON_COMPARTMENT_SUB,

    # Parameter classification
    PARAMETER_TYPE = "Stressor",
    MEASURED_TYPE = "Concentration",

    # Sample information
    SUBSAMPLE,
    SAMPLE_ID,

    # Measurement values
    MEASURED_FLAG = vm_convert_operator(Operator),
    MEASURED_VALUE = Verdi,
    MEASURED_UNIT = vm_convert_unit(Unit_Name),
    MEASURED_N = Ant_verdier,

    # Uncertainty (not reported in Vannmiljø)
    UNCERTAINTY_TYPE = "Not reported",
    UNCERTAINTY_UPPER = NA_real_,
    UNCERTAINTY_LOWER = NA_real_,

    # Detection limits
    LOQ_VALUE = Kvantifiseringsgrense,
    LOQ_UNIT = vm_convert_unit(Unit_Name),
    LOD_VALUE = Deteksjonsgrense,
    LOD_UNIT = vm_convert_unit(Unit_Name),

    # TODO: Fix methods reference
    # Protocol references (FIXME: Need proper protocol ID mapping)
    SAMPLING_PROTOCOL = "1",
    EXTRACTION_PROTOCOL = "2",
    FRACTIONATION_PROTOCOL = "3",
    ANALYTICAL_PROTOCOL = "4",

    # Comments
    MEASUREMENT_COMMENT = Kommentar,

    .keep = "none"
  )

# Validate against eData schema
edata_measurements <- initialise_measurements_tibble() |>
  add_row(edata_measurements)

message(glue(
  "Created measurements table: {nrow(edata_measurements)} measurements"
))
```

# Export

## Create Export

We export the resulting data to a zip file in our

```{r}
#| label: export-create-zip

# Assemble datasets for export
export_datasets <- list(
  edata_sites = edata_sites,
  edata_campaign = edata_campaign,
  edata_reference = edata_reference,
  edata_parameters = edata_parameters,
  edata_samples = edata_samples,
  edata_biota = edata_biota,
  edata_measurements = edata_measurements
)

# Export as ZIP
# TODO: Disabled
if (
  !file.exists(here(
    "data/raw/eData",
    paste0(config$metadata$campaign_short, ".zip")
  ))
) {
  timestamp <- format(Sys.time(), "%Y%m%d_%H%M%S")

  export_path <- export_campaign_zip(
    dataset_list = export_datasets,
    campaign_name = config$metadata$campaign_short,
    output_path = here(
      "data",
      "raw",
      "eData",
      glue("{config$metadata$campaign_short}_{timestamp}.zip")
    )
  )
  message(glue("Export complete: {export_path}"))
} else {
  message("Export file already exists, skipping")
}

```

## Validate Export

Do some quick checks to see if the data we're exporting looks like we expect it to.

```{r}
#| label: validate_export

# TODO: We need to expand this session massively without treading on the toes of the main analysis pathway. How?
edata_measurements |> count(ENVIRON_COMPARTMENT)


edata_measurements |> count(MEASURED_UNIT)
# nuts
count(vm_filtered, Unit_Name)
# the old classic lower case l for Liter
```

```{r}
#| label: fig-vm-dotplot

edata_measurements |>
  ggplot(
    aes(
      y = MEASURED_VALUE,
      x = SAMPLING_DATE |> as.Date(),
      colour = ENVIRON_COMPARTMENT_SUB
    )
  ) +
  geom_point(shape = 1) +
  facet_wrap(
    facets = vars(MEASURED_UNIT),
    nrow = 2,
    ncol = 2,
    scales = "free_y"
  )
```

@fig-vm-dotplot looks, frankly, very suspicious.

## Species Data Coverage

None of these are very informative yet, because we don't filter down to stuff that's definitely biota first.

```{r}
#| label: eda-species-coverage

# Species group coverage
species_group_coverage <- vm_merged |>
  group_by(SPECIES_GROUP_medium) |>
  reframe(count = n()) |>
  mutate(
    flag = case_when(
      SPECIES_GROUP_medium %in% c("Not reported", "", NA) ~ "Missing",
      .default = NA_character_
    )
  ) |>
  arrange(desc(count))

datatable(species_group_coverage, caption = "Species Group Coverage")

# Sample species coverage
sample_species_coverage <- vm_merged |>
  group_by(SAMPLE_SPECIES_medium) |>
  reframe(count = n()) |>
  mutate(
    flag = case_when(
      SAMPLE_SPECIES_medium %in% c("Not reported", "", NA) ~ "Missing",
      .default = NA_character_
    )
  ) |>
  arrange(desc(count))

datatable(sample_species_coverage, caption = "Sample Species Coverage")

# Species gender coverage
species_gender_coverage <- vm_merged |>
  group_by(SPECIES_GENDER_medium) |>
  reframe(count = n()) |>
  mutate(
    flag = case_when(
      SPECIES_GENDER_medium %in% c("Not reported", "", NA) ~ "Missing",
      .default = NA_character_
    )
  ) |>
  arrange(desc(count))

datatable(species_gender_coverage, caption = "Species Gender Coverage")

# Sample tissue coverage
sample_tissue_coverage <- vm_merged |>
  group_by(SAMPLE_TISSUE_medium) |>
  reframe(count = n()) |>
  mutate(
    flag = case_when(
      SAMPLE_TISSUE_medium %in% c("Not reported", "", NA) ~ "Missing",
      .default = NA_character_
    )
  ) |>
  arrange(desc(count))

datatable(sample_tissue_coverage, caption = "Sample Tissue Coverage")
```